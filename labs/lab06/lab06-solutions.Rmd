---
title: "ETC3555 2018 - Lab 6"
author: "Cameron Roach and Souhaib Ben Taieb"
date: "29 August 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
subtitle: Neural networks and backpropagation
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'h')
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

rm(list=ls())
```

### Exercise 1

We will consider a dataset which contains 5000 training examples of handwritten digits (from 0 to 9), where each training example is a 20 pixels by 20 pixels grayscale image of the digit. Each pixel indicate the grayscale intensity at that location in the image. We have vectorized the 20 by 20 grid of pixels into a 400-dimensional vector. This gives us a 5000 by 400 input matrix *X* where each row is a training example for a handwrittend digit image. The output vector is a 5000 dimensional vector *y* where the digits "1" to "9" are labeled as "1" to "9" while a "0" is labeled "10". 

The following code visualizes some training examples

```{r}
source("plotDigits.R")
load("digits.Rdata") # load X and y
X <- cbind(1, X)
n <- nrow(X)
plotDigits(X[sample(n, 12), -1])
```


Consider a neural network with an input layger, one hidden layer and a output layer with 10 units (corresponding to the 10 digit classes). Do not forget the extra bias units which always outputs +1. A logistic activation function will be used for all units. This neural network can be used for multi-class classification. The prediction for $x$ will be the label that has the largest output. In other words, we assign class C to $x$ where $C = \text{argmax}_{k \in \{1, 2, \dots, 10\}} h_k(x)$ and $h_k(x)$ is the value of the $k$th output unit.


Complete the function *feedforward_predict*  which apply forward propagation to compute the prediction for $x$. This function will take the following arguments:

1. `x`: a vector of dimension $(d^{(0)} + 1) \times 1$.
2. `W1`: a weight matrix of dimension $(d^{(0)} + 1) \times d^{(1)}$.
3. `W2`: a weight matrix of dimension $(d^{(1)} + 1) \times d^{(2)}$.

(Note: We will use the matrix notations presented in Lecture 12)

```{r}
sigmoid <- function(z) {
  g <- 1 / (1 + exp(-1 * z))
  g
}

feedforward_predict <-  function(x, W1, W2){
  x0 <- x
  s1 <- t(W1) %*% x0
  x1 <- rbind(1, sigmoid(s1))
  s2 <- t(W2) %*% x1
  x2 <- sigmoid(s2)
  h <- which.max(x2) 
  h
}
```

The file *weights.Rdata* provides the network parameters $W_1$ and $W_2$ already trained by us. You should see that the classification accuracy is about 97.5% for the training data.

```{r}
load("weights.Rdata")
W1_given <- t(W1)
W2_given <- t(W2)

pred <- sapply(seq(nrow(X)), function(i){
   feedforward_predict( matrix(X[i, ], ncol = 1)  , W1_given, W2_given)
})

print(paste("Training Set Accuracy: ", mean(pred == y) * 100))
```

The following code can be used to visualize some predictions of the neural networks.

```{r}
id_missclass <- which(y != pred)
id_class <- which(y == pred)

ids <- c(sample(id_missclass, 3), sample(id_class, 3))

for (i in  ids){
  plotDigits(X[i, -1])
  print(paste("Neural network prediction : ", pred[i] , " (digit ", pred[i]%%10, ")", " - ", y[i], sep = ""))
}
```

### Exercise 2
 
Consider the following networks, and suppose we want to minimize squared errors losses, i.e. $e = (y - h(\mathbf{x}))^2$.
 
 
 ![Source: Abu-Mostafa et al. Learning from data. AMLbook.](./network-ex.jpg)
 
(Note: We will use the matrix notations presented in Lecture 12)
 
 1. What are the values of $W^{(1)}$, $W^{(2)}$ and $W^{(3)}$?
 
```{r}
 W1 <- cbind(c(0.1, 0.3), c(0.2, 0.4))
 W2 <- matrix(c(0.2, 1, -3), ncol = 1)
 W3 <- matrix(c(1, 2), ncol = 1) 
 ```
 
 2. Run forward propagation for the data point x = -2. What are the values of $\mathbf{x}^{(0)}$, $\mathbf{s}^{(1)}$, $\mathbf{x}^{(1)}$, $\mathbf{s}^{(2)}$, $\mathbf{x}^{(2)}$, $\mathbf{s}^{(3)}$, $\mathbf{x}^{(3)}$?
 
```{r eval = TRUE}
x <- -2
x0 <- matrix(c(1, x), ncol = 1)
s1 <- t(W1) %*% x0
x1 <- rbind(1, tanh(s1))
s2 <- t(W2) %*% x1
x2 <- rbind(1, tanh(s2))
s3 <- t(W3) %*% x2
x3 <- tanh(s3)

print(x0)
print(s1)
print(x1)
print(s2)
print(x2)
print(s3)
print(x3)
```
 
3. Run backward propagation for the data point x = -2, y = 2. What are the values of $\mathbf{\delta}^{(3)}$, $\mathbf{\delta}^{(2)}$ and $\mathbf{\delta}^{(1)}$?
```{r}
y <- 2
delta3 <- 2 * (x3 - y) * (1 - x3^2)
theta_prime_s2 <- 1 - tail(x2 * x2, - 1) 
delta2 <- theta_prime_s2 *  tail(W3 %*%  delta3, - 1)
theta_prime_s1 <- 1 - tail(x1 * x1, - 1) 
delta1 <- theta_prime_s1 *  tail(W2 %*%  delta2, - 1)  

print(delta3)
print(delta2)
print(delta1)
```

4. What are the values of $\frac{\partial e}{\partial W^{(1)}}$, $\frac{\partial e}{\partial W^{(2)}}$ and $\frac{\partial e}{\partial W^{(3)}}$?

```{r}
gW1 <- x0 %*% t(delta1)
gW2 <- x1 %*% t(delta2)
gW3 <- x2 %*% t(delta3)

print(gW1)
print(gW2)
print(gW3)

```

5. Repeat the computations for the case when the output transformation is the identity.

Replace the following lines in the previous code

```{r}
x3 <- s3
delta3 <- 2 * (x3 - y) 
```