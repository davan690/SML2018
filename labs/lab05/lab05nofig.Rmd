---
title: "ETC3555 2018 - Lab 5"
author: "Cameron Roach and Souhaib Ben Taieb"
date: "17 August 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
subtitle: Linear models and gradient descent (II/II)
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'h')
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

rm(list=ls())
```

### Assignment - Question 1

Solve exercise 3.10 in Learning From Data.

### Assignment - Question 2

Solve problem 3.4 in Learning From Data (problem 1.5 is given below). 

### Assignment - Question 3

Complete the code for the function `Ein_linreg` which computes the in-sample error of a linear regression fit based on the weight vector `w`. This function will take the following arguments:

1. `X`: an input matrix of dimension $n \times p$.
2. `y`: a response vector of dimension $n \times 1$.
3. `w`: a weight vector of dimension $p \times 1$.


```{r}
Ein_linreg <- function(X, y, w){
  
}
```

Write a similar function for logistic regression.

```{r}
Ein_logreg <- function(X, y, w){
  
}
```

For both linear regression and logistic regression, write a function that computes the gradient of the function `Ein` at `w`.


```{r}
gEin_linreg <- function(X, y, w){
  
}
```

```{r}
gEin_logreg <- function(X, y, w){
  
}
```

Complete the code of the function `GD` which applies gradient descent to the function `Ein` starting from `w0`. The function will take the following arguments:

1. `X`: an input matrix of dimension $n \times p$.
2. `y`: a response vector of dimension $n \times 1$.
3. `Ein`: a function which takes arguments `X`, `y` and `w`, and computes the in-sample error for `w`.
4. `gEin`: a function which takes arguments `X`, `y` and `w`, and computes the gradient of Ein at `w`.
5. `w0`: the initial weights
6. `eta`: the learning rate
7. `precision`: a small value 
8. `nb_iters`: the maximum number of iterations

The function will stop when $|Ein(w(t + 1)) - Ein(w(t))| < precision$ or when $t = nb\_iters$


```{r eval=FALSE}
GD <- function(X, y, Ein, gEin, w0, eta, precision, nb_iters){
  allw <- vector("list", nb_iters) 
  cost <- numeric(nb_iters)
  allw[[1]] <- w0
  cost[1] <- Ein(X, y, allw[[1]])
  
  
  
  list(allw = allw, cost = cost)
}
```

For linear regression, try your functions on the following example. 

```{r eval=FALSE}
set.seed(1900)
# Function taken from Friedman et al.
genx <- function(n,p,rho){
  #    generate x's multivariate normal with equal corr rho
  # Xi = b Z + Wi, and Z, Wi are independent normal.
  # Then Var(Xi) = b^2 + 1
  #  Cov(Xi, Xj) = b^2  and so cor(Xi, Xj) = b^2 / (1+b^2) = rho
  z <- rnorm(n)
  if(abs(rho) < 1){
    beta <- sqrt(rho/(1-rho))
    x <- matrix(rnorm(n*p), ncol=p)
    A <- matrix(rnorm(n), nrow=n, ncol=p, byrow=F)
    x <- beta * A + x
  }
  if(abs(rho)==1){ x=matrix(rnorm(n),nrow=n,ncol=p,byrow=F)}
  
  return(x)
}


N <- 100
p <- 10
rho <- 0.2
X <- genx(N, p, rho)
w_true <- ((-1)^(1:p))*exp(-2*((1:p)-1)/20)
eps <- rnorm(N)
k <- 3
y <- X %*% w_true + k * eps

res <- GD(X, y, Ein_linreg, gEin_linreg, rep(0, p), 0.01, 0.0001, 100)
plot(res$cost)

print(w_true)
print(unlist(tail(res$allw, 1)))
```

Try different values of `eta`, `precision` and `nb_iters`, and run the example again with your best values. Compare your solution with the closed-form solution.

For logistic regression, try your functions on the following example.

```{r eval=FALSE}
set.seed(1900)
N <- 100
l <- -5; u <- 5
x <- seq(l, u, by = 0.1)
w_true <- matrix(c(-3, 1, 1), ncol = 1)
a <- -w_true[2]/w_true[3]
b <- -w_true[1]/w_true[3]


X0 <- matrix(runif(2 * N, l, u), ncol = 2)
X <- cbind(1, X0)
y <- sign(X %*% w_true)



res <- GD(X, y, Ein_logreg, gEin_logreg, rep(0, 3), 0.05, 0.0001, 500)
plot(res$cost)

print(w_true)
w_best <- unlist(tail(res$allw, 1))
print(w_best)

plot(c(l, u), c(u, l), type = 'n', xlab = "x1", ylab = "x2")
lines(x, a*x +b)
points(X0, col = ifelse(y == 1, "red", "blue"))

a_best <- -w_best[2]/w_best[3]
b_best <- -w_best[1]/w_best[3]
lines(x, a_best*x + b_best, col = "red")

```

### Exercise

Update your functions to implement stochastic gradient descent, and include $L_2$ regularization for both linear and logistic regression

## TURN IN 

- Your `.Rmd` file (which should knit without errors and without assuming any packages have been pre-loaded)
- Your Word (or pdf) file that results from knitting the Rmd.
- DUE: August 28, 11:55pm (late submissions not allowed), loaded into moodle
